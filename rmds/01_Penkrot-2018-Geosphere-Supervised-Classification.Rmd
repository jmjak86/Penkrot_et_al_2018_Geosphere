---
title: "Penkrot et al. 2018 Geosphere U1419 Supervised Classification Model"
author: "John M. Jaeger"
date: "March 20, 2018"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(error = TRUE)
```


The primary goal of this code is to test whether these sedimentary properties can discriminate amongst the different litofacies.
This code will import elemental concentrations and physical property data from from Integrated Ocean Drilling Site U1419. See here for more details about this drilling location: http://iodp.tamu.edu/scienceops/expeditions/alaska_tectonics_climate.html 
 
Code is available on Github at: https://github.com/jmjak86/Penkrot_et_al_2018_Geosphere

Code generated by:
John M. Jaeger
Associate Professor
241 Williamson Hall
P.O. Box 112120
Dept. of Geological Sciences
University of Florida
Gainesville FL 32611-2120, USA
(352) 846-1381
ORCID ID# orcid.org/0000-0003-0248-489X
http://people.clas.ufl.edu/jmjaeger/

last edited: 20180320

#Code Information
This code will import physical property data and normalized scanning XRF elemental concentrations (NMS normalized; Lyle et al., 2012) from Integrated Ocean Drilling Site U1419, and then perform two types of supevised classification on these data. See here for more details about this drilling location: http://iodp.tamu.edu/scienceops/expeditions/alaska_tectonics_climate.html. 

* The datasets come from samples analyzed in the Department of Geological Sciences at the University of Florida and published values from:
Walczak, M. H., Mix, A. C., Willse, T., Slagle, A., Stoner, J. S., Jaeger, J., … Kioka, A. (2015). Correction of non-intrusive drill core physical properties data for variability in recovered sediment volume. Geophysical Journal International, 202(2), 1317–1323. https://doi.org/10.1093/gji/ggv204

### Load packages
```{r load_packages, message=FALSE, results='hide'}
library(plyr)
library(dplyr)
library(psych)
library(caret)
library(car)
library(robCompositions)
library(klaR)
library(e1071)
```



## Import the data
```{r load_data}
# load the dataset
U1419_all <- read.csv("../raw_data/2018-03-20_U1419-Penkrot_Geosphere-2018-data.csv") 
U1419_all<-U1419_all[,1:15]
U1419.all<- tbl_df(U1419_all)
```
Data are in the following units (nms=normalized median-scaled method; Lyle et al., 2012):
Al (mass% nms);	Ca (mass% nms);	Zr (ppm nms);	K (mass% nms);	Rb (mass% nms);	Si (mass% nms);	b_star (unitless);	NGR (cps/g vol. normalized);	MS (cm^3/g vol. normalized)

## Data preparation
We remove outlier values because they have strong influence on variance-related analyses. Because the data include compositional parameters (i.e., elemental abundances), we use a robust routine from the robCompositions package (Filzmoser, P., & Hron, K. (2008). Outlier detection for compositional data using robust methods. Mathematical Geosciences, 40(3), 233–248. https://doi.org/10.1007/s11004-007-9141-5).
```{r outlier detection}

U1419.all[,11] <- U1419.all[,11]+10 # adjust b* so it has only positive values
nRows <- nrow(U1419.all)
nCols <- ncol(U1419.all)
U1419.mud <- U1419.all[1:508,]
U1419.diamict <- U1419.all[509:nRows,]

mudout <- outCoDa(U1419.mud[5:13], quantile = 0.975, method = "robust", h = 1/2, coda=log)
plot(mudout,which=2)
mud.outclean <- U1419.mud[!mudout$outlierIndex,]
diaout <- outCoDa(U1419.diamict[5:13], quantile = 0.975, method = "robust", h = 1/2, coda=log)
plot(diaout,which=2)
diamict.outclean <- U1419.diamict[!diaout$outlierIndex,]

U1419.allClean <- rbind(mud.outclean,diamict.outclean)
U1419.diamict <- U1419.allClean[509:nRows,]
```

## Data Inspection
###Near Zero Values
The next step is to remove any constant and almost constant predictors across samples (called zero and near-zero variance predictors) using the the function nearZeroVar from the caret package does. It not only removes predictors that have one unique value across samples (zero variance predictors), but also removes predictors that have both 1) few unique values relative to the number of samples and 2) large ratio of the frequency of the most common value to the frequency of the second most common value (near-zero variance predictors). 

```{r nearZeroVar}
nearZeroVar(U1419.allClean[,5:13], saveMetrics = TRUE)
nearZeroVar(U1419.diamict[,5:13], saveMetrics = TRUE)
```
The results show that all variables do not have zer-zero variance so they are all included in the following data preparation steps.

###Kruskal-Wallis tests
We first perform a non-parametric Kruskal-Wallis test on each property to see if the Mud and Diamict lithofacies have unique values following Collins, A. L., Walling, D. E., & Leeks, G. J. L. (1998). Use of composite fingerprints to determine the provenance of the contemporary suspended sediment load transported by rivers. Earth Surface Processes and Landforms, 23(1), 31–52. https://doi.org/10.1002/(SICI)1096-9837(199801)23:1<31::AID-ESP816>3.0.CO;2-Z
```{r Kruskal-Wallis test Mud/Diamict}
#Kruskal-Wallis test to see if lithofacies are unique; if P-value <<.05 significance level, we conclude that samples are nonidentical populations.

U1419.allClean$Mud_Diamict <- as.factor(U1419.allClean$Mud_Diamict)

kruskal.test(U1419.allClean$Al, U1419.allClean$Mud_Diamict)
kruskal.test(U1419.allClean$Ca, U1419.allClean$Mud_Diamict)
kruskal.test(U1419.allClean$Zr, U1419.allClean$Mud_Diamict)
kruskal.test(U1419.allClean$K, U1419.allClean$Mud_Diamict)
kruskal.test(U1419.allClean$Rb, U1419.allClean$Mud_Diamict)
kruskal.test(U1419.allClean$Si, U1419.allClean$Mud_Diamict)
kruskal.test(U1419.allClean$b_star, U1419.allClean$Mud_Diamict)
kruskal.test(U1419.allClean$NGR, U1419.allClean$Mud_Diamict)
kruskal.test(U1419.allClean$MS, U1419.allClean$Mud_Diamict)
```
All sedimentary properties are distinctive at p=0.05 for the Mud-Diamict binary lithofacies model.

We now repeat the same test for the Diamict-only samples.
```{r Kruskal-Wallis test Diamict Only}
#Kruskal-Wallis test to see if lithofacies are unique; if P-value <<.05 significance level, we conclude that samples are nonidentical populations.

U1419.allClean$Diamict_only <- as.factor(U1419.allClean$Diamict_only)

kruskal.test(U1419.diamict$Al, U1419.diamict$Diamict_only)
kruskal.test(U1419.diamict$Ca, U1419.diamict$Diamict_only)
kruskal.test(U1419.diamict$Zr, U1419.diamict$Diamict_only)
kruskal.test(U1419.diamict$K, U1419.diamict$Diamict_only)
kruskal.test(U1419.diamict$Rb, U1419.diamict$Diamict_only)
kruskal.test(U1419.diamict$Si, U1419.diamict$Diamict_only)
kruskal.test(U1419.diamict$b_star, U1419.diamict$Diamict_only)
kruskal.test(U1419.diamict$NGR, U1419.diamict$Diamict_only)
kruskal.test(U1419.diamict$MS, U1419.diamict$Diamict_only)
```
All sedimentary properties are distinctive at p=0.05 for the Diamict-only lithofacies model.

###Data Correlation
The next step is to eliminate highly correlated elements following methodology here: https://topepo.github.io/caret/pre-processing.html
```{r correlation_check}
#Identifying Correlated Predictors in Mud/diamict data
AllCor <-  cor(U1419.allClean[,5:13],use="pairwise.complete.obs")
hc <- findCorrelation(as.matrix(AllCor), cutoff=0.9) # putt any value as a "cutoff"
hc <- sort(hc)
print(AllCor)
if(length(hc)==0){
  print(AllCor)
}else{
  print(AllCor[-hc,-hc])
}
# no parameters are correlated at >0.9

#Identifying Correlated Predictors in Diamict-only data
DiaCor <-  cor(U1419.diamict[,5:13],use="pairwise.complete.obs")
hc2 <- findCorrelation(as.matrix(DiaCor), cutoff=0.9) # putt any value as a "cutoff"
hc2 <- sort(hc2)
print(DiaCor)
if(length(hc2)==0){
  print(DiaCor)
}else{
  print(DiaCor[-hc2,-hc2])
}
# no parameters are significantly correlated
```
No parameters are positvely correlated at >0.9 so all will be used in subsequent analyses.

###Levine Test for data distribution
The next processing step is to determine which type of discriminant analyses to conduct (linear or quadratic) to test for discrimation power of the chosen properties This is done with the Levene test, which is less sensistive to non-normality of data following methods here: http://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm.
For this section, only one element is analyzed (Ca). Replace Ca with other parameters to test.
```{r variance_homogeneity Mud-Diamict}
## Tests for Homogeneity of variance, run for each element ----------------------------------------------------
leveneTest(Ca~ Mud_Diamict, data=U1419.allClean)#p value >0.05 means they are homogeneous; if not homogeneous, you cannot use LDA
```
Ca, Zr, K, Rb, Si, MS do not have homogeneous variance in the mud-diamict dataset, so a quadratic discrimination analysis will be used

```{r variance_homogeneity Diamict-only}
## Tests for Homogeneity of variance, run for each element ----------------------------------------------------
leveneTest(Ca~ Diamict_only, data=U1419.diamict)#p value >0.05 means they are homogeneous; if not homogeneous, you cannot use LDA
```
Al,  Zr, K, Rb, Si, b_star, NGR, MS do not have homogeneous variance in the diamict-only dataset, so a quadratic discrimination analysis will be used

##Data Transformation-Full Dataset
The next step is to transform and scale data. A center-log ratio transformation is chosen for elemental data to remove constant-sum effects (Templ, M., Filzmoser, P., & Reimann, C. (2008). Cluster analysis applied to regional geochemical data: Problems and possibilities. Applied Geochemistry, 23(8), 2198–2213. https://doi.org/10.1016/j.apgeochem.2008.03.004). 
```{r clr_scale}
mudout <- U1419.allClean[which(U1419.allClean$Mud_Diamict== "mud"), ]
diamictout <- U1419.allClean[which(U1419.allClean$Mud_Diamict== "diamict"), ]

dataCLRmud <- cenLR(mudout[,5:10]) #clr transformed elemental data
dataCLRdiamict <- cenLR(diamictout[,5:10]) #clr transformed elemmental data

mudCLR <- dataCLRmud$x.clr
diamictCLR <- dataCLRdiamict$x.clr

allelementdata.CLR <- rbind(mudCLR,diamictCLR)
ppdata <- U1419.allClean[,13:nCols-2]

allelements.norm <- apply(allelementdata.CLR, MARGIN = 2, FUN = function(X) (X - min(X))/diff(range(X)))
ppdataA.norm  <- apply(U1419.allClean[,13:nCols-2], MARGIN = 2, FUN = function(X) (X - min(X))/diff(range(X)))

lith1 <- U1419.allClean[,3]
lith2 <- U1419.allClean[,4]
alldata.norm<- cbind(lith1,allelements.norm,ppdataA.norm)

diamictdata<- cbind(lith2,allelementdata.CLR,ppdata)
nRows <- nrow(diamictdata)
diamictdataD<- diamictdata[509:nRows,]
diamictdata.norm1 <- apply(diamictdataD[,2:10], MARGIN = 2, FUN = function(X) (X - min(X))/diff(range(X)))
diamictdata.norm <- cbind(lith2[509:nRows,],diamictdata.norm1)
```


## Quadradic Discrimination Analysis of Mud-Diamict Lithofacies
The next step is to select the elements that best discriminate amongst the groups. The Greedy Wilks approach is used following methods of Gorman Sanisaca, L.E., Gellis, A.C., and Lorenz, D.L., 2017, Determining the sources of fine-grained sediment using the Sediment Source Assessment Tool (Sed_SAT): U.S. Geological Survey Open File Report 2017–1062, 104 p., https://doi.org/10.3133/ofr20171062
```{r greedy_wilks}
gw_obj<- greedy.wilks(Mud_Diamict~., data=alldata.norm[,1:10], niveau = 0.05)## 'niveau' is probabilty that addition of variable does not contribute to model
gw_obj
```

The next step is to test the discrimination power of the combination of the elements chosen from the Greedy Wilks routine.
```{r discriminant_analysis Model 1}
Group_mod1All<-dplyr::select(alldata.norm,Mud_Diamict,Al,NGR,Rb,Si,MS,b_star,Ca)
# Select Training and Testing subsets -------------------------------------
data <- Group_mod1All
nColsF <- ncol(data)
set.seed(2969)
sample.ind = sample(2, 
                    nrow(data),
                    replace = T,
                    prob = c(0.25,0.75))
data.test = data[sample.ind==1,]#data.dev
data.train = data[sample.ind==2,]#data.val
dataD.train <-data.train 
#See how balanced the test & training sets look as Group; training set should be balanced
table(data$Mud_Diamict)/nrow(data)
table(data.test$Mud_Diamict)/nrow(data.test)
table(data.train$Mud_Diamict)/nrow(data.train)

#if one of the training groups is too large, you need to resample using Caret
set.seed(9560)
dtrainCols <- ncol(data.train)

down_train <- downSample(x = data.train[, 1:dtrainCols],
                         y = data.train$Mud_Diamict)
table(down_train$Mud_Diamict)/nrow(down_train)

dataD.train <- down_train[,1:dtrainCols]

# Discriminant Testing ---------------------------------------------------
qda.fit <- qda(Mud_Diamict ~., data=dataD.train)
qda.fit
qda.class <- predict(qda.fit, data.test)$class
qda1.table <- table(qda.class, data.test$Mud_Diamict)
qda1.table
mean(qda.class == data.test$Mud_Diamict)

fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 100)
set.seed(825)
qdaFit1 <- train(Mud_Diamict~ ., data = dataD.train, 
                 method = "qda", 
                 trControl = fitControl,
                 finalModel=TRUE,
                 verbose = FALSE,
                 na.action = na.omit)
qdaFit1
qdaFit1$finalModel

confusionMatrix(data.test$Mud_Diamict, predict(qdaFit1, data.test))

#relative importance of variables in model
qdaImp <- varImp(qdaFit1, scale = FALSE)
qdaImp
```

## Support Vector Machine Classification Analysis of Mud-Diamict Lithofacies
The SVM design follows Masaaki Tsujitani and Yusuke Tanaka, “Cross-Validation, Bootstrap, and Support Vector Machines,” Advances in Artificial Neural Systems, vol. 2011, Article ID 302572, 6 pages, 2011. doi:10.1155/2011/302572
```{r SVM Mud-diamict}
Group_mod1All<-dplyr::select(alldata.norm,Mud_Diamict,Al,NGR,Rb,Si,MS,b_star,Ca)

## svm >
svm.model1 <- svm(Mud_Diamict~ ., data = dataD.train, cost = 100, gamma = 1)
svm.pred1 <- predict(svm.model1, data.test)
## compute svm confusion matrix > 
svmtable1 <- table(pred = svm.pred1, data.test$Mud_Diamict)
svmtable1

classAgreement(svmtable1,qda1.table)

## compute rpart confusion matrix > 
fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,#10
  ## repeated ten times
  repeats = 100)#100
set.seed(645)
#fitControl <- trainControl(number = 200)
SVMFit1 <- train(Mud_Diamict~ ., data = dataD.train, 
                 method = "svmRadial", 
                 trControl = fitControl,
                 tuneLength = 3,
                 finalModel=TRUE,
                 verbose = FALSE,
                 scaled = FALSE,
                 na.action = na.omit)
SVMFit1
SVMFit1$finalModel

confusionMatrix(data.test$Mud_Diamict, predict(SVMFit1, data.test))

#relative importance of variables in model
SVMImp1 <- varImp(SVMFit1, scale = FALSE)
SVMImp1
```

## Quadradic Discrimination Analysis of Diamict-only Lithofacies
```{r greedy_wilks Diamict Only}
diamictsel <- subset(diamictdata.norm, select=c("Diamict_only", "Al", "Ca","Zr","Rb","Si","MS","b_star","NGR")) #K and Rb are too correlated and routine crashes; Rb only is used

gw_obj2<- greedy.wilks(Diamict_only~., data=diamictsel, niveau = 0.05)## 'niveau' is probabilty that addition of variable does not contribute to model
gw_obj2
```

The next step is to test the discrimination power of the combination of the elements chosen from the Greedy Wilks routine.
```{r discriminant_analysis Model 2, warning=FALSE}
Group_mod2All<-dplyr::select(diamictdata.norm,Diamict_only,Si,Zr,b_star,Al,NGR,MS,Ca,Rb)

# Select Training and Testing subsets -------------------------------------
data <- Group_mod2All
nColsF <- ncol(data)
set.seed(2969)
sample.ind = sample(2, 
                    nrow(data),
                    replace = T,
                    prob = c(0.25,0.75))
data.test = data[sample.ind==1,]#data.dev
data.train = data[sample.ind==2,]#data.val

# #See how balanced the test & training sets look as Group; training set should be balanced
table(data$Diamict_only)/nrow(data)
table(data.test$Diamict_only)/nrow(data.test)
table(data.train$Diamict_only)/nrow(data.train)

#if one of the training groups is too large, you need to resample using Caret
set.seed(9560)
dtrainCols <- ncol(data.train)

down_train <- downSample(x = data.train[, 1:dtrainCols],
                         y = data.train$Diamict_only)
table(down_train$Diamict_only)/nrow(down_train)

dataD.train <- down_train[,1:dtrainCols]

# Discriminant Testing ---------------------------------------------------
qda.fit2 <- qda(Diamict_only ~., data=dataD.train)
qda.fit2

qda.class <- predict(qda.fit2, data.test)$class
qda.table <- table(qda.class, data.test$Diamict_only)
qda.table
mean(qda.class == data.test$Diamict_only)

fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,
  ## repeated ten times
  repeats = 100)
set.seed(825)
qdaFit3 <- train(Diamict_only~ ., data = dataD.train, 
                 method = "qda", 
                 trControl = fitControl,
                 finalModel=TRUE,
                 verbose = FALSE,
                 na.action = na.omit)
qdaFit3
qdaFit3$finalModel

confusionMatrix(data.test$Diamict_only, predict(qdaFit3, data.test))

#relative importance of variables in model
qdaImp <- varImp(qdaFit3, scale = FALSE)
qdaImp
```

## Support Vector Machine Classification Analysis of Diamict-only Lithofacies
```{r SVM models Diamict-only}
## svm > 
svm.model <- svm(Diamict_only ~ ., data = dataD.train, cost = 100, gamma = 1)
svm.pred <- predict(svm.model, data.test)
## compute svm confusion matrix > 
svmtable <- table(pred = svm.pred, data.test$Diamict_only)
svmtable

classAgreement(svmtable,qda.table)

## compute rpart confusion matrix > 
fitControl <- trainControl(## 10-fold CV
  method = "repeatedcv",
  number = 10,#10
  ## repeated ten times
  repeats = 100)#100
set.seed(645)
#fitControl <- trainControl(number = 200)
SVMFit2 <- train(Diamict_only~ ., data = dataD.train, 
                 method = "svmRadial", 
                 trControl = fitControl,
                 tuneLength = 12,
                 finalModel=TRUE,
                 verbose = FALSE,
                 scaled = FALSE,
                 na.action = na.omit)
SVMFit2
SVMFit2$finalModel

confusionMatrix(data.test$Diamict_only, predict(SVMFit2, data.test))

#relative importance of variables in model
SVMImp <- varImp(SVMFit2, scale = FALSE)
SVMImp
```
